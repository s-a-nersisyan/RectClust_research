\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{authblk}
\usepackage{cite}
\usepackage{graphicx}
%\usepackage{MnSymbol}
\usepackage{float}
%\usepackage{makecell}
\usepackage{multirow}
%\pagestyle{empty}
\usepackage[left=2cm,right=2cm,
    top=1cm,bottom=2cm,bindingoffset=0cm]{geometry}
    
\title{EM-algorithm for mixture of generalized Gaussian distribution (GGD)}
\author{}
\date{}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{restrict}{Restriction}
\newtheorem{proposition}{Proposition}
\newtheorem{consequence}{Corollary}

\theoremstyle{plain}
\newtheorem{statement}{Утверждение}
\newtheorem{theorem}{Theorem}
 

\begin{document}

\maketitle
\thispagestyle{empty}

\section{Definitions and problem statement}
Density of GGD:
$$
GND(x | \mu, \alpha, \beta) = \frac{\beta}{2 \alpha \Gamma(1/\beta)} 
e^{-(|x - \mu|/\alpha)^{\beta}}.
$$

Mixture of GGDs:
$$
p(x | \theta) = \sum_{k = 1}^{K} \pi_k p_k(x | \theta_k)
$$

Maximum a posteriori estimation (MAP):
$$
p(\theta | D) \propto 
\prod_{i = 1}^N p(x_i | \theta) \cdot 
\prod_{k = 1}^K p(\theta_k | \omega) \xrightarrow{\theta} \max,
$$

with independent components:
$$
p_k(x) = \prod_{j = 1}^{d} GND(x^j | \mu^j_k, \alpha^j_k, \beta),
$$

$$
\log p_k(x_i) = d \log \left( \frac{\beta}{2 \Gamma(1/\beta)} \right) - 
\sum_{j=1}^d \log \alpha_k^j - 
\sum_{j=1}^d \left( \frac{|x_i^j - \mu_k^j|}{\alpha_k^j} \right)^\beta .
$$

Prior distribution on rectangle side lengths:
$$
p(\alpha_k | a, b) = \prod_{j = 1}^d p(\alpha^j_k | a, b),
$$

$$
p(\alpha | a, b) \propto \alpha^{-1 - a} e^{-b / \alpha^\beta}.
$$

\section{EM-algorithm}

Aim: to minimize Negative Log-Likelihood:
$$
NLL(\theta) = -\log p(\theta | D) = -\sum_{i = 1}^N \log{ \left( \sum_{k = 1}^{K} \pi_k p_k(x_i | \theta_k) \right)}
$$

\subsection{Definitions}
\begin{itemize}
\item $X = (x_i^j)$ -- matrix $N \times d$;
\item $p = (p_k(x_i)) = (p_1, \ldots, p_K)$ -- matrix $N \times K$;
\item $\pi = (\pi_k) = (\pi_1, \ldots, \pi_K)$ -- matrix $1 \times K$;
\item $\mu = (\mu_k^j)$ -- matrix $K \times d$;
\item $\alpha = (\alpha_k^j)$ -- matrix $K \times d$;
\item $r = (r_{ik})$ -- matrix $N \times K$.
\end{itemize}

\subsection{E-step}
$$
r_{ik} = \frac{\pi^t_k p_k(x_i | \theta^t_k)}
{\sum_{k' = 1}^K \pi^t_{k'} p_{k'}(x_i | \theta^t_{k'})};
$$

In matrix language:
\begin{enumerate}
\item $\widetilde{r} = p \cdot \text{diag}(\pi) = (p_1 \pi_1, \ldots, p_K \pi_K) = 
(\widetilde{r}_1, \ldots, \widetilde{r}_N)^T$ --
matrix $N \times K$;
\item $u = \log \widetilde{r} = (\log p_1 + \log \pi_1, \ldots, \log p_K + \log \pi_K) =
(u_1, \ldots, u_N)^T$.
\item $r = (\widetilde{r}_1 / \sum \widetilde{r}_1, \ldots, 
\widetilde{r}_N / \sum \widetilde{r}_N)^T$;
\item $\log r =  (u_1 - \log \left(\sum e^{u_1}\right), \ldots, 
u_N - \log \left(\sum e^{u_N})\right)$.
\end{enumerate}

\subsection{M-step}

To simplify notation we use $r_k = \sum_{i=1}^N r_{ik}$

Auxiliarly function:
\begin{align*}
Q(\theta | \theta^t) &= 
-\sum_{i = 1}^N \sum_{k = 1}^K r_{ik} \log{\pi_k} 
-\sum_{i = 1}^N \sum_{k = 1}^K r_{ik} \log{p_k (x_i | \theta_k)} = \\
&= -\sum_{i = 1}^N \sum_{k = 1}^K r_{ik} \log{\pi_k} - 
N d \log \left( \frac{\beta}{2 \Gamma(1/\beta)} \right) + 
\sum_{k = 1}^K \sum_{j=1}^d \sum_{i = 1}^N r_{ik} \left( 
 \log \alpha_k^j + 
\left( \frac{|x_i^j - \mu_k^j|}{\alpha_k^j} \right)^\beta
\right) = \\
&= -\sum_{i = 1}^N \sum_{k = 1}^K r_{ik} \log{\pi_k} -
N d \log \left( \frac{\beta}{2 \Gamma(1/\beta)} \right) + 
\sum_{k = 1}^K \sum_{j=1}^d \left( 
r_k \log \alpha_k^j + 
\frac{1}{(\alpha_k^j)^\beta} \sum_{i = 1}^N r_{ik} \left|x_i^j - \mu_k^j\right|^\beta
\right);
\end{align*}

% \sum_{k = 1}^K \log{p(\alpha_k | a, b)};

It should be minimized with respect to $\theta$ which yields:

$$
\pi_k = \frac{r_k}{N};
$$

$$
\mu^j_k = \arg \min m_k^j(t_k^j) = \sum_{i = 1}^N r_{ik} |x_i^j - t_k^j|^\beta ;
$$

$$
\alpha^j_k = \left( \beta~\frac{m_k^j (\mu^j_k) }{r_k} \right)^{1 / \beta}
$$


In case of $\beta = 2$ it turns to
$$
\mu^j_k = \frac{\sum_{i = 1}^N r_{ik} x_i^j}{r_k}.
$$

Легко видеть, что для минимизации $l(\mu, \alpha)$ достаточно сначала минимизировать
её по $\mu$, а затем по $\alpha$. Минимизация по $\mu$ экивалентна минимизации 
$m(\mu)$, которая является выпуклой функцией, принимающей на $\pm \infty$ значение
$+ \infty$, а значит, достигающей единственной точки минимума. Более того, очевидно,
что данная точка минимума лежит на отрезке 
$[\min x_i, \max x_i]$. Значит, функционал $m(\mu)$ может быть эффективно минимизирован,
например, методом золотого сечения. Обозначим минимальное 
значение $m(\mu)$ как $m_*$, после чего остается минимизировать функцию
$$
l(\alpha) = (N + a + 1)\log{\alpha} + \frac{m_* + b}{\alpha^\beta},
$$
что может быть легко проделано аналитически и приводит к следующему результату:

\begin{align*}
\alpha_*^\beta &= \frac{\beta(m_* + b)}{N + a + 1} = 
\left(1 - \frac{a + 1}{N + a + 1} \right) \frac{\beta m_*}{N} +
\frac{a + 1}{N + a + 1} \cdot \frac{\beta b}{a + 1}.
\end{align*}

Перепараметризуя гиперпараметры априорного распределения с помощью формул
$$
\alpha_0^\beta = \frac{\beta b}{a + 1}, \lambda = \frac{a + 1}{N + a + 1},
$$
и используя обозначение
$$\alpha_{MLE} = \left(\frac{\beta m_*}{N}\right)^{1 / \beta},$$
получаем окончательное выражение:
$$
\alpha_* = 
\left((1 - \lambda) \alpha_{MLE}^\beta + \lambda \alpha_0^\beta \right)^{1 / \beta}.
$$

Stop condition:
$$
\frac{\left|p(\theta^{t+1} | D) - p(\theta^{t} | D)\right|}{p(\theta^{t} | D)} < \varepsilon
$$
		
		
\end{document}
